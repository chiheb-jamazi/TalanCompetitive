{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, sys, time\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "from time import sleep \n",
    "from selenium.common.exceptions import NoSuchAttributeException\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from hdfs import InsecureClient\n",
    "import csv\n",
    "import re\n",
    "\n",
    "def semrush(comp):\n",
    "    xx=comp+\".com\"\n",
    "    websites=[]\n",
    "    websites.append(xx)\n",
    "    browser = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "    sleep(2)\n",
    "    browser.maximize_window()\n",
    "    sleep(3)\n",
    "    src = browser.page_source\n",
    "    soup = BeautifulSoup(src, 'lxml')\n",
    "    browser.get('https://www.semrush.com/analytics/overview/?searchType=domain&q='+websites[0])\n",
    "    sleep(7)\n",
    "    browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    username='omhiri43@gmail.com'\n",
    "    password='omar20mhiri'\n",
    "    browser.find_element_by_css_selector('#srf-limit-popup > div > div > div > div > div.sso-limit-inner > div.sso-block.sso-block_state_limit-popup > div > div > div > div.sso-tabs > div:nth-child(2)').click()\n",
    "    elementID = browser.find_element_by_name('email')\n",
    "    elementID.send_keys(username)\n",
    "    sleep(2)\n",
    "    elementID = browser.find_element_by_name('password')\n",
    "    elementID.send_keys(password)\n",
    "    sleep(2)\n",
    "    elementID.submit()\n",
    "    sleep(7)\n",
    "    Backlinks_number=browser.find_element_by_css_selector('#domain-overview-app > div > div > div.report\\.module__reportCnt___1cf7L > div.___SBoxSizing_1tphm-red-team.summary_desktop\\.module__summary___tY5bb.___SCard_wa8fh_gg_ > div > div:nth-child(1) > div:nth-child(4) > div:nth-child(2) > a > span').text\n",
    "    src = browser.page_source\n",
    "    soup = BeautifulSoup(src, 'lxml')\n",
    "    kui=soup.find_all('span',({'class':'___SText_ey63k_gg_ _size_200_ey63k_gg_ __color_ey63k_gg_'}))\n",
    "    countries_percentage=[]\n",
    "    countries_percentages=[]\n",
    "    for i in range(1,5) :\n",
    "        k=kui[i].get_text().split()[0]\n",
    "        countries_percentage.append(k)\n",
    "    countries_percentages.append(countries_percentage)\n",
    "    sleep(2)\n",
    "    browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    sleep(1)\n",
    "    browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    organic_search_traffics=[]\n",
    "    organic_search_traffic=browser.find_element_by_css_selector('#domain-overview-app > div > div > div.report\\.module__reportCnt___1cf7L > div.___SBoxSizing_1tphm-red-team.summary_desktop\\.module__summary___tY5bb.___SCard_wa8fh_gg_ > div > div:nth-child(1) > div:nth-child(2) > div:nth-child(2) > a > span').text\n",
    "    organic_search_traffics.append(organic_search_traffic)\n",
    "    organic_competitorss=[]\n",
    "    organic_competitors=[]\n",
    "    k=[]\n",
    "    browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    sleep(1)\n",
    "    browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    try : \n",
    "        pi=browser.find_elements_by_class_name('___SText_13ebd_gg_')\n",
    "        for i in pi [30:60] :\n",
    "            k.append(i.text)\n",
    "        for i in k : \n",
    "            if (i[0].isalpha()) and (i[:2]!='ht') and (i[:2]!='Vi') and ('.' in i):\n",
    "                organic_competitors.append(i)\n",
    "    except : pass\n",
    "    organic_competitorss.append(organic_competitors)\n",
    "    sleep(1)\n",
    "    organic_keywordss=[]\n",
    "    organic_keywords=[]\n",
    "    keywords=[]\n",
    "    try : \n",
    "                pi =browser.find_elements_by_class_name('___SText_13ebd_gg_')\n",
    "                for i in pi[15:30] :\n",
    "                        keywords.append(i.text)\n",
    "                for i in keywords : \n",
    "                        if (i[0].isalpha()) and (i[:2]!='ht') and (i[:2]!='Vi') and((i[:2]!='Sh')):\n",
    "                            organic_keywords.append(i)\n",
    "    except : pass\n",
    "    browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "    organic_keywordss.append(organic_keywords)\n",
    "    backlinkss=[]\n",
    "    backlinks=[]\n",
    "    p_keywords=[]\n",
    "    try : \n",
    "                pi =browser.find_elements_by_class_name('___SText_13ebd_gg_')\n",
    "                for i in pi :\n",
    "                        p_keywords.append(i.text)\n",
    "                for i in p_keywords : \n",
    "                        if (i[:2]=='ht') and ( websites[0] not in i):\n",
    "                            backlinks.append(i)\n",
    "    except : pass\n",
    "    backlinkss.append(backlinks)\n",
    "    countriess=[]\n",
    "    countries=[]\n",
    "    try : \n",
    "                khh= browser.find_elements_by_css_selector(\"[data-at='country-code']\")\n",
    "                for i in khh :\n",
    "                    countries.append(i.text)\n",
    "    except : pass\n",
    "    countriess.append(countries)\n",
    "    ###\n",
    "    ###\n",
    "    ###\n",
    "    informations = ({'wesbtites': websites, 'organic_search_traffic': organic_search_traffics,'Backlinks_number':Backlinks_number, 'countries': countriess,'countries_percentage':countries_percentages, 'backlinks': backlinkss, \n",
    "                     'organic_keywords': organic_keywordss, 'organic_competitors':organic_competitorss}) \n",
    "    df = pd.DataFrame(informations)\n",
    "     \n",
    "    company=comp\n",
    "    file_name= company+'_semrush_df'\n",
    "    hdfs_url = \"http://localhost:9870/\"\n",
    "    hdfs_user = \"chiheb\"\n",
    "    c = InsecureClient(hdfs_url, user=hdfs_user)\n",
    "    c.makedirs(\"/user/chiheb/\"+company) \n",
    "    with c.write(company+'/'+file_name, encoding = 'utf-8') as writer:\n",
    "        df.to_csv(writer,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
