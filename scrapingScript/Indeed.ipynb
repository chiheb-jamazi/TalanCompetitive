{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, sys, time\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep \n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import pandas as pd\n",
    "from getpass import getpass\n",
    "from time import sleep \n",
    "from selenium.common.exceptions import NoSuchAttributeException\n",
    "import csv\n",
    "import re\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "def indeed(comp):\n",
    "        oppa=comp\n",
    "        global websites \n",
    "        websites=[]\n",
    "        websites.append(oppa)\n",
    "        s=websites[0]\n",
    "        company=s\n",
    "        browser = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "        sleep(2)\n",
    "        browser.get('https://www.indeed.com/cmp/'+s)\n",
    "        sleep(5)\n",
    "        browser.maximize_window()\n",
    "        sleep(1)\n",
    "        src = browser.page_source\n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "        global jobss\n",
    "        jobss=[]\n",
    "        jobs=[]\n",
    "        jobs_number=[]\n",
    "        src = browser.page_source\n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "        h=soup.find_all('a',{'class':'css-ku00p5 emf9s7v0'})\n",
    "        r=0\n",
    "        for k in h:\n",
    "            try :\n",
    "                s=h[r].find_all('div')\n",
    "                for i in s[0] :\n",
    "                    jobs.append(i.get_text().strip())\n",
    "                r=r+1\n",
    "            except : pass\n",
    "        jobs\n",
    "        for i in range(len(jobs)):\n",
    "            if i%2==0:\n",
    "                jobss.append(jobs[i])\n",
    "            else :\n",
    "                jobs_number.append(jobs[i])   \n",
    "        browser.get('https://www.indeed.com/cmp/'+company+'/jobs')\n",
    "        sleep(1)\n",
    "        src = browser.page_source\n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "        jobListItem_title=[]\n",
    "        jobListItem_location=[]\n",
    "        jobListItem_tags=[]\n",
    "        for i in range(1):\n",
    "            try :\n",
    "                    src = browser.page_source\n",
    "                    soup = BeautifulSoup(src, 'lxml')\n",
    "                    s=browser.find_elements_by_css_selector(\"[data-testid='jobListItem-title']\")\n",
    "                    for i in s:\n",
    "                        jobListItem_title.append(i.text)\n",
    "                    src = browser.page_source\n",
    "                    soup = BeautifulSoup(src, 'lxml')\n",
    "                    s=browser.find_elements_by_css_selector(\"[data-testid='jobListItem-location']\")\n",
    "                    for i in s:\n",
    "                        jobListItem_location.append(i.text)\n",
    "                    src = browser.page_source\n",
    "                    soup = BeautifulSoup(src, 'lxml')\n",
    "                    s=browser.find_elements_by_css_selector(\"[data-testid='jobListItem-tags']\")\n",
    "                    for i in s:\n",
    "                        jobListItem_tags.append(i.text)\n",
    "            except : pass\n",
    "        browser.get('https://www.indeed.com/cmp/'+company+'/reviews'+'?fcountry=ALL')\n",
    "        sleep(1)\n",
    "        src = browser.page_source\n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "        try : \n",
    "            s=soup.find('div',{'data-testid':'reviewsList'})\n",
    "            kh=s.find_all('div')\n",
    "        except : pass\n",
    "        ################################reviews:\n",
    "        browser.get('https://www.indeed.com/cmp/'+company+'/reviews'+'?fcountry=ALL')\n",
    "        sleep(1)\n",
    "        src = browser.page_source\n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "        global reviewsss\n",
    "        global ratingsss\n",
    "        global author_reviewsss\n",
    "        reviewsss=[]\n",
    "        ratingsss=[]\n",
    "        authors_reviewsss=[]\n",
    "        reviewss=[]\n",
    "        ratingss=[]\n",
    "        reviewss=[]\n",
    "        authors_reviewss=[]\n",
    "        reviews_headlines=[]\n",
    "        reviews_description=[]\n",
    "        try : \n",
    "            for xyz in range(10) : \n",
    "                try :\n",
    "                    #reviews:\n",
    "                    ratings=[]\n",
    "                    s=soup.find_all('div',{'class':'css-1h3aion eu4oa1w0'})\n",
    "                    for i in s:\n",
    "                        k=i.get_text().strip()\n",
    "                        ratingss.append(k[0:3])\n",
    "                    ssh=browser.find_elements_by_css_selector(\"[data-testid='title']\")\n",
    "                    for i in ssh :\n",
    "                        reviews_headlines.append(i.text)\n",
    "                    ssh=browser.find_elements_by_css_selector(\"[data-tn-component='reviewDescription']\")\n",
    "                    for i in ssh :\n",
    "                        reviews_description.append(i.text)\n",
    "                    sx=soup.find_all('span',{'class':'css-xvmbeo e1wnkr790'})\n",
    "                    for i in sx:\n",
    "                        if len(i)>=10:\n",
    "                            authors_reviewss.append(i.get_text().strip())\n",
    "                    browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "                    browser.find_element_by_css_selector(\"[title='Next']\").click()\n",
    "                    sleep(1)\n",
    "                except : break\n",
    "        except : pass\n",
    "        reviewsss.append(reviewss)\n",
    "        ratingsss.append(ratingss)\n",
    "        authors_reviewsss.append(authors_reviewss)\n",
    "        browser.get('https://www.indeed.com/cmp/'+company+'/salaries')\n",
    "        sleep(1)\n",
    "        src = browser.page_source\n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "        try :\n",
    "            ssh=browser.find_element_by_css_selector(\"[data-tn-section='salary-category-summary-v2']\").text\n",
    "        except : pass\n",
    "        global jobs_names\n",
    "        jobs_names=[]\n",
    "        try:\n",
    "            k=soup.find_all('span',{'class':\"cmp-SalaryCategoryCard-title css-153b0q2 e1wnkr790\"})\n",
    "            for i in k :\n",
    "                jobs_names.append(i.get_text().strip())\n",
    "        except : pass \n",
    "        global jobs_salaries\n",
    "        jobs_salaries=[]\n",
    "        try : \n",
    "            k=soup.find_all('span',{'class':\"css-1s341tc e1wnkr790\"})\n",
    "            for i in k :\n",
    "                jobs_salaries.append(i.get_text().strip())\n",
    "        except : pass \n",
    "        interviews = ({'ratings': ratingss,'reviews_headlines': reviews_headlines,'reviews_description': reviews_description, 'authors_reviews': authors_reviewss })\n",
    "        available_jobs = ({'jobListItem_title': jobListItem_title,'jobListItem_location': jobListItem_location,\n",
    "                 'jobListItem_tags': jobListItem_tags})\n",
    "        jobs_categories=({'jobs':jobss,'jobs_number':jobs_number})\n",
    "        Salaries=({'jobs_names':jobs_names, 'jobs_salaries':jobs_salaries})       \n",
    "        global df\n",
    "        df = pd.DataFrame(interviews)\n",
    "        global df1\n",
    "        df1 = pd.DataFrame(available_jobs)\n",
    "        global df3\n",
    "        df3 = pd.DataFrame(jobs_categories)\n",
    "        global df4\n",
    "        df4 = pd.DataFrame(Salaries)\n",
    "        \n",
    "        company=comp\n",
    "        file_name= company+'_ind_interviews_df'\n",
    "        hdfs_url = \"http://localhost:9870/\"\n",
    "        hdfs_user = \"chiheb\"\n",
    "        c = InsecureClient(hdfs_url, user=hdfs_user)\n",
    "        c.makedirs(\"/user/chiheb/\"+company) \n",
    "        with c.write(company+'/'+file_name, encoding = 'utf-8') as writer:\n",
    "            df.to_csv(writer,index=False)\n",
    "            \n",
    "        file_name2= company+'_ind_jobs_df'\n",
    "        with c.write(company+'/'+file_name2, encoding = 'utf-8') as writer:\n",
    "            df3.to_csv(writer,index=False)\n",
    "        \n",
    "        file_name3= company+'_ind_saleries_df'\n",
    "        with c.write(company+'/'+file_name3, encoding = 'utf-8') as writer:\n",
    "            df4.to_csv(writer,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
