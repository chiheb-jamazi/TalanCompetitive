{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accenture.com\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 101.0.4951\n",
      "Get LATEST chromedriver version for 101.0.4951 google-chrome\n",
      "Driver [/home/chiheb/.wdm/drivers/chromedriver/linux64/101.0.4951.41/chromedriver] found in cache\n",
      "/home/chiheb/anaconda3/envs/scraping/lib/python3.7/site-packages/ipykernel_launcher.py:44: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "/home/chiheb/anaconda3/envs/scraping/lib/python3.7/site-packages/ipykernel_launcher.py:45: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "/home/chiheb/anaconda3/envs/scraping/lib/python3.7/site-packages/ipykernel_launcher.py:48: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'organic_search_traffics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c2638806c451>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mcountriess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m informations = ({'wesbtites': websites, 'organic_search_traffic': organic_search_traffics, 'countries': countriess,  'backlinks': backlinkss, \n\u001b[0m\u001b[1;32m    108\u001b[0m                  'organic_keywords': organic_keywordss, 'organic_competitors':organic_competitorss}) \n\u001b[1;32m    109\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minformations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'organic_search_traffics' is not defined"
     ]
    }
   ],
   "source": [
    "import os, random, sys, time\n",
    "from urllib.parse import urlparse\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.wait import WebDriverWait\n",
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import time\n",
    "import pandas as pd\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from hdfs import InsecureClient\n",
    "# from selenium import webdriver\n",
    "# from bs4 import BeautifulSoup\n",
    "# from time import sleep \n",
    "# from selenium.webdriver.common.keys import Keys\n",
    "# import pandas as pd\n",
    "# from getpass import getpass\n",
    "# from time import sleep \n",
    "# from selenium.common.exceptions import NoSuchAttributeException\n",
    "import csv\n",
    "import re\n",
    "files=open('untitled.txt')\n",
    "lines=files.readlines()\n",
    "xx=input()\n",
    "websites=[]\n",
    "websites.append(xx)\n",
    "browser = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "sleep(2)\n",
    "browser.maximize_window()\n",
    "sleep(1)\n",
    "src = browser.page_source\n",
    "soup = BeautifulSoup(src, 'lxml')\n",
    "for i in range( 0, 3, 2 ) : \n",
    "    try : \n",
    "        browser.get('https://www.semrush.com/analytics/overview/?searchType=domain&q='+websites[0])\n",
    "        sleep(2)\n",
    "        browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "        username=lines[0]\n",
    "        password=lines[1]\n",
    "        browser.find_element_by_css_selector('#srf-limit-popup > div > div > div > div > div.sso-limit-inner > div.sso-block.sso-block_state_limit-popup > div > div > div > div.sso-tabs > div:nth-child(2)').click()\n",
    "        elementID = browser.find_element_by_name('email')\n",
    "        elementID.send_keys(username)\n",
    "        sleep(2)\n",
    "        elementID = browser.find_element_by_name('password')\n",
    "        elementID.send_keys(password)\n",
    "        sleep(2)\n",
    "        elementID.submit()\n",
    "        sleep(2)\n",
    "        browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "        sleep(1)\n",
    "        browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "        organic_search_traffics=[]\n",
    "        organic_search_traffic=browser.find_element_by_css_selector('#domain-overview-app > div > div > div.report\\.module__reportCnt___1cf7L > div.___SBoxSizing_1tphm-red-team.summary_desktop\\.module__summary___tY5bb.___SCard_wa8fh_gg_ > div > div:nth-child(1) > div:nth-child(2) > div:nth-child(2) > a > span').text\n",
    "        organic_search_traffics.append(organic_search_traffic)\n",
    "        organic_competitorss=[]\n",
    "        organic_competitors=[]\n",
    "        k=[]\n",
    "        browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight)\")\n",
    "        sleep(1)\n",
    "        try : \n",
    "            pi=browser.find_elements_by_class_name('___SText_13ebd_gg_')\n",
    "            for i in pi [30:60] :\n",
    "                    k.append(i.text)\n",
    "            for i in k : \n",
    "                if (i[0].isalpha()) and (i[:2]!='ht') and (i[:2]!='Vi') and ('.' in i):\n",
    "                    organic_competitors.append(i)\n",
    "        except : pass\n",
    "        organic_competitorss.append(organic_competitors)\n",
    "        sleep(1)\n",
    "        organic_keywordss=[]\n",
    "        organic_keywords=[]\n",
    "        keywords=[]\n",
    "        try : \n",
    "            pi =browser.find_elements_by_class_name('___SText_13ebd_gg_')\n",
    "            for i in pi[15:30] :\n",
    "                    keywords.append(i.text)\n",
    "            for i in keywords : \n",
    "                    if (i[0].isalpha()) and (i[:2]!='ht') and (i[:2]!='Vi') and((i[:2]!='Sh')):\n",
    "                        organic_keywords.append(i)\n",
    "        except : pass\n",
    "        organic_keywordss.append(organic_keywords)\n",
    "        backlinkss=[]\n",
    "        backlinks=[]\n",
    "        p_keywords=[]\n",
    "        try : \n",
    "            pi =browser.find_elements_by_class_name('___SText_13ebd_gg_')\n",
    "            for i in pi :\n",
    "                    p_keywords.append(i.text)\n",
    "            for i in p_keywords : \n",
    "                    if (i[:2]=='ht') and ( websites[0] not in i):\n",
    "                        backlinks.append(i)\n",
    "        except : pass\n",
    "        backlinkss.append(backlinks)\n",
    "        countriess=[]\n",
    "        countries=[]\n",
    "        try : \n",
    "            khh= browser.find_elements_by_css_selector(\"[data-at='country-code']\")\n",
    "            for i in khh :\n",
    "                countries.append(i.text)\n",
    "        except : pass\n",
    "        countriess.append(countries)\n",
    "    except : pass \n",
    "informations = ({'wesbtites': websites, 'organic_search_traffic': organic_search_traffics, 'countries': countriess,  'backlinks': backlinkss, \n",
    "                 'organic_keywords': organic_keywordss, 'organic_competitors':organic_competitorss}) \n",
    "df = pd.DataFrame(informations)\n",
    "df           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "company='accenture'\n",
    "file_name= company.upper()+'_Semrush_df'\n",
    "hdfs_url = \"http://localhost:9870/\"\n",
    "hdfs_user = \"chiheb\"\n",
    "c = InsecureClient(hdfs_url, user=hdfs_user)\n",
    "c.makedirs(\"/user/chiheb/accenture\") \n",
    "with c.write('accenture/'+file_name, encoding = 'utf-8') as writer:\n",
    "    df.to_csv(writer,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x2ce8df01a00>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymongo\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient()\n",
    "client = MongoClient(\"mongodb://localhost:27017\")\n",
    "db=client[\"pfe\"]\n",
    "data=df.to_dict(orient =\"records\")\n",
    "db.semrush.insert_many(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
